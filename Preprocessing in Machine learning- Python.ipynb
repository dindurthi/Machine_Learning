{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing in sklearn Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation (mean removal and variance scaling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. The formula for standardisation is \n",
    "\\begin{align}\n",
    "\\dot{x}&=\\frac{x-\\mu}{\\sigma}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Standardisation__\n",
    "\n",
    "_Standardize across entire space_:Calculate the mean/std (1 values) for the entire matrix and subtract/divide this element wise for each cell.\n",
    "\n",
    "_Standardize on row/input case level_: Calculate the mean/std for each entire row, and subtract this element wise on each features in that row.\n",
    "\n",
    "_Standardize on column/feature basis_: Calculate the mean/std for each entire column (feature), and subtract/divide this element wise all cells in that column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning algorithms work on data that is centered around zero and have variance in the same order.If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected._scale_ is the function from python used to calculate the standardisation.\n",
    "\n",
    "__Scale function:__\n",
    "\n",
    "    sklearn.preprocessing.scale(X, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "__Parameters:__\n",
    "\n",
    "__X : {array-like, sparse matrix}__\n",
    "The data to center and scale.\n",
    "\n",
    "__axis : int (0 by default)__\n",
    "axis used to compute the means and standard deviations along. If 0, independently standardize each feature, otherwise (if 1) standardize each sample.\n",
    "\n",
    "__with_mean : boolean, True by default__\n",
    "If True, center the data before scaling.\n",
    "\n",
    "__with_std : boolean, True by default__\n",
    "If True, scale the data to unit variance (or equivalently, unit standard deviation).\n",
    "\n",
    "__copy : boolean, optional, default True__\n",
    "set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSC matrix and if axis is 1).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train=np.array([[1.,-1.,2.],[2.,0.,0.],[0.,1.,-1.]])\n",
    "X_scaled=preprocessing.scale(X_train)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled=preprocessing.scale(X_train)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation of mean and the standard deviation is carried along each feature(column) on using scale function as shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = (X_train - np.mean(X_train,axis=0)) / np.std(X_train,axis=0)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler \n",
    "\n",
    "Standard Scaler is an API that has the same functionality as scale.It has an estimator associated with it. The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. \n",
    "\n",
    "    class sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "__Parameters:__\n",
    " \n",
    "__copy : boolean, optional, default True__\n",
    "If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.\n",
    "\n",
    "__with_mean : boolean, True by default__\n",
    "If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.\n",
    "\n",
    "__with_std : boolean, True by default__\n",
    "If True, scale the data to unit variance (or equivalently, unit standard deviation)\n",
    "\n",
    "\n",
    "\n",
    "__Attributes:__\n",
    "\n",
    "   __scale_ : ndarray or None, shape (n_features,)__\n",
    "Per feature relative scaling of the data. This is calculated using np.sqrt(var_). Equal to None when with_std=False.\n",
    "\n",
    "__mean_ : ndarray or None, shape (n_features,)__\n",
    "The mean value for each feature in the training set. Equal to None when with_mean=False.\n",
    "\n",
    "__var_ : ndarray or None, shape (n_features,)__\n",
    "The variance for each feature in the training set. Used to compute scale_. Equal to None when with_std=False.\n",
    "\n",
    "__n_samples_seen_ : int or array, shape (n_features,)__\n",
    "The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls.\n",
    "\n",
    "__Methods:__\n",
    "1. fit(X[, y])                 - Compute the mean and std to be used for later scaling.\n",
    "2. fit_transform(X[, y])\t    -Fit to data, then transform it.\n",
    "3. get_params([deep])\t        -Get parameters for this estimator.\n",
    "4. inverse_transform(X[, copy])\t-Scale back the data to the original representation\n",
    "5. partial_fit(X[, y])\t        -Online computation of mean and std on X for later scaling.\n",
    "6. set_params(**params)\t        -Set the parameters of this estimator.\n",
    "7. transform(X[, y, copy])\t    -Perform standardization by centering and scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81649658, 0.81649658, 1.24721913])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling features to a range\n",
    "\n",
    "An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler, respectively.\n",
    "\n",
    "The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\n",
    "\n",
    "#### MinMaxScaler\n",
    "\n",
    "    class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    Transforms features by scaling each feature to a given range.\n",
    "\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.Transforms features by scaling each feature to a given range.\n",
    "\n",
    "The transformation is given by:\n",
    "\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "\n",
    "This transformation is often used as an alternative to zero mean, unit variance scaling.\n",
    "\n",
    "__Parameters:__\n",
    "__feature_range : tuple (min, max), default=(0, 1)__\n",
    "Desired range of transformed data.\n",
    "\n",
    "__copy : boolean, optional, default True__\n",
    "Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array).\n",
    "\n",
    "\n",
    "\n",
    "__Attributes:__\n",
    "\n",
    "__min_ : ndarray, shape (n_features,)__\n",
    "Per feature adjustment for minimum. Equivalent to min - X.min(axis=0) * self.scale_\n",
    "\n",
    "__scale_ : ndarray, shape (n_features,)__\n",
    "Per feature relative scaling of the data. Equivalent to (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "__data_min_ : ndarray, shape (n_features,)__\n",
    "Per feature minimum seen in the data\n",
    "\n",
    "__data_max_ : ndarray, shape (n_features,)__\n",
    "Per feature maximum seen in the data\n",
    "\n",
    "__data_range_ : ndarray, shape (n_features,)__\n",
    "Per feature range (data_max_ - data_min_) seen in the data\n",
    "\n",
    "__Methods:__\n",
    "1. fit(X[, y])\t-Compute the minimum and maximum to be used for later scaling.\n",
    "2. fit_transform(X[, y])\t-Fit to data, then transform it.\n",
    "3. get_params([deep])\t-Get parameters for this estimator.\n",
    "4. inverse_transform(X)\t-Undo the scaling of X according to feature_range.\n",
    "5. partial_fit(X[, y])\t-Online computation of min and max on X for later scaling.\n",
    "6. set_params(**params)\t-Set the parameters of this estimator.\n",
    "7. transform(X)\t-Scaling features of X according to feature_range.\n",
    "\n",
    "\n",
    "__Here is an example to scale a toy data matrix to the [0, 1] range:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         1.        ]\n",
      " [1.         0.5        0.33333333]\n",
      " [0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_std = (X_train- X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "print(X_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAbs Scaler\n",
    "\n",
    "MaxAbsScaler works in a very similar fashion, but scales in a way that the training data lies within the range [-1, 1] by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data.\n",
    "\n",
    "     class sklearn.preprocessing.MaxAbsScaler(copy=True)\n",
    "Scale each feature by its maximum absolute value.\n",
    "\n",
    "This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.\n",
    "\n",
    "This scaler can also be applied to sparse CSR or CSC matrices.\n",
    "\n",
    "__Parameters:__\t\n",
    "__copy : boolean, optional, default is True__\n",
    "Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).\n",
    "\n",
    "__Attributes:__\t\n",
    "__scale_ : ndarray, shape (n_features,)__\n",
    "Per feature relative scaling of the data.\n",
    "New in version 0.17: scale_ attribute.\n",
    "\n",
    "__max_abs_ : ndarray, shape (n_features,)__\n",
    "Per feature maximum absolute value.\n",
    "\n",
    "__n_samples_seen_ : int__\n",
    "The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across partial_fit calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaler\n",
    "\n",
    "    class sklearn.preprocessing.RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
    "    \n",
    " Scale features using statistics that are robust to outliers.\n",
    "\n",
    "This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "\n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.\n",
    "\n",
    "\n",
    "__Parameters:__\n",
    "__with_centering : boolean, True by default__\n",
    "If True, center the data before scaling. This will cause transform to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.\n",
    "\n",
    "__with_scaling : boolean, True by default__\n",
    "If True, scale the data to interquartile range.\n",
    "\n",
    "__quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0__\n",
    "Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calculate scale_.\n",
    "\n",
    "New in version 0.18.\n",
    "\n",
    "__copy : boolean, optional, default is True__\n",
    "If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.\n",
    "\n",
    "__Attributes:__\t\n",
    "center_ : array of floats\n",
    "The median value for each feature in the training set.\n",
    "\n",
    "__scale_ : array of floats__\n",
    "The (scaled) interquartile range for each feature in the training set.\n",
    "\n",
    "New in version 0.17: scale_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.        ,  1.33333333],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [-1.        ,  1.        , -0.66666667]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robscaler=preprocessing.RobustScaler()\n",
    "transformer =robscaler.fit_transform(X_train)\n",
    "transformer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
